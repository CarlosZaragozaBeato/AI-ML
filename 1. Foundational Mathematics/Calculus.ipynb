{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed mpmath-1.3.0 sympy-1.13.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sympy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "A derivative represents the rate at which a function changes as its input changes. In ML, derivatives are crucial for optimization algorithms like gradient descent, which aim to minimize loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F: x**2 + 3*x + 2\n",
      "Derivative: 2*x + 3\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Define a symbolic variable and function\n",
    "x = sp.symbols('x')\n",
    "f =  x**2 + 3*x + 2 \n",
    "\n",
    "print(f\"F: {f}\")\n",
    "\n",
    "# Compute the derivative\n",
    "f_prime = sp.diff(f, x)\n",
    "print(f\"Derivative: {f_prime}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrals\n",
    "\n",
    "Integrals are calculate the area under a curve, representing the accumulation of quantities. While less common than derivatives in ML, integrals are used in areas like probabilistic models and continuos distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indefinite Integral: x**3/3\n"
     ]
    }
   ],
   "source": [
    "# Define a symbolic function \n",
    "\n",
    "f = x ** 2\n",
    "\n",
    "# Compute the indefinite integral\n",
    "integral_f = sp.integrate(f, x)\n",
    "print(f\"Indefinite Integral: {integral_f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partial Derivatives\n",
    "\n",
    "Partial derivatives involve differentiating functions with multiple variables with respect to one variable at a time. They are essential in optimizing functions of several variables, such as in training Ml models with multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partial derivative of x:2*x\n",
      "Partial derivative of y:2*y\n"
     ]
    }
   ],
   "source": [
    "# Define the symbolic variables and functons \n",
    "y = sp.symbols('y')\n",
    "f = x** 2 + y**2\n",
    "\n",
    "# Compute partial derivatives\n",
    "f_partial_x = sp.diff(f, x)\n",
    "f_partial_y = sp.diff(f, y)\n",
    "\n",
    "print(f\"Partial derivative of x:{f_partial_x}\")\n",
    "print(f\"Partial derivative of y:{f_partial_y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "\n",
    "Gradient descnt is an optimization algorithm that uses derivatives to interatively adjust parameters, minimizing a loss function. The gradient vector, composed of partial derivatives, indicates the direction of the steepest ascent; moving in the opposite leads to the minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8 0.8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "\n",
    "def gradient(x, y):\n",
    "    return np.array([2*x, 2*y])\n",
    "\n",
    "params = np.array([1.0, 1.0])\n",
    "learning_rate = 0.1\n",
    "\n",
    "# Perform one iteration of gradient descent\n",
    "params = params - learning_rate * gradient(params[0], params[1])\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule\n",
    "\n",
    "The chain rule is used to compute derivatives of a composition function. In AI, it's fundamental in backpropagation for training neural networks, allowing, the calculation of gradient through layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*x*cos(x**2)\n"
     ]
    }
   ],
   "source": [
    "u = sp.symbols('u')\n",
    "g = x ** 2\n",
    "h = sp.sin(u)\n",
    "composite_function = h.subs(u, g)\n",
    "\n",
    "derivative_composite = sp.diff(composite_function, x)\n",
    "print(derivative_composite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariable Calculus \n",
    "\n",
    "Multivariable Calculus extends calculus to functions of several variables. It's essential in ML for understanding and optimizing functions with multiple inputs, such as in multivariate regression and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2*x + y, x + 2*y]\n"
     ]
    }
   ],
   "source": [
    "f = x**2 + y ** 2 + x*y \n",
    "\n",
    "gradient_f = [sp.diff(f, var) for var in (x, y)]\n",
    "print(gradient_f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
